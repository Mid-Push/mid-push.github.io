---
layout: homepage
---

Hi! I’m a PhD student at <a href="https://www.cmu.edu/" style="color:#71b07b;">Carnegie Mellon University</a>, advised by <a href="https://www.andrew.cmu.edu/user/kunz1/" style="color:#71b07b;">Prof. Kun Zhang</a>.  
I have also been fortunate to intern at <a href="https://research.adobe.com/" style="color:#71b07b;">Adobe Research</a> with <a href="https://sites.google.com/site/zhelin625/" style="color:#71b07b;">Zhe Lin</a>, <a href="https://zzutk.github.io/" style="color:#71b07b;">Zhifei Zhang</a>, and <a href="https://www.tobiashinz.com/" style="color:#71b07b;">Tobias Hinz</a>, and at <a href="https://research.google/" style="color:#71b07b;">Google</a> with <a href="https://sites.google.com/view/zhao-yang/" style="color:#71b07b;">Yang Zhao</a>, <a href="https://xavierxiao.github.io/" style="color:#71b07b;">Zhisheng Xiao</a>, and <a href="https://ckkelvinchan.github.io/" style="color:#71b07b;">Kelvin C. K. Chan</a>.

---

## Research Highlights

My long-term research goal is to develop generative AI systems that go beyond producing realistic multi-modal data to capturing the underlying structure of the world. By embedding causal principles into machine learning and computer vision, I aim to make generative models not only powerful, but also controllable, interpretable, trustworthy, and capable of systematic generalization.

In my recent work, I have explored this vision through several directions:

- **Text-to-image and video generation**: advancing controllable and scalable text-driven image and video synthesis [<a href="https://openreview.net/pdf?id=hUHRTaTfvZ" style="color:#71b07b;">ICML ’25</a>, <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.pdf" style="color:#71b07b;">CVPR ’25</a>, <a href="https://arxiv.org/pdf/2312.03771" style="color:#71b07b;">arXiv ’23</a>, <a href="https://arxiv.org/pdf/2212.05034.pdf" style="color:#71b07b;">CVPR ’23</a>, <a href="https://arxiv.org/pdf/2502.02690" style="color:#71b07b;">arXiv ’25</a>].  
- **Conditional image generation**: designing methods that flexibly adapt to conditioning signals [<a href="https://arxiv.org/pdf/2212.05034.pdf" style="color:#71b07b;">CVPR ’23</a>, <a href="https://openreview.net/pdf?id=U2g8OGONA_V" style="color:#71b07b;">ICLR ’23</a>, <a href="https://openreview.net/pdf?id=RNZ8JOmNaV4" style="color:#71b07b;">NeurIPS ’22</a>, <a href="https://arxiv.org/pdf/2306.12511.pdf" style="color:#71b07b;">NeurIPS ’23</a>, <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Maximum_Spatial_Perturbation_Consistency_for_Unpaired_Image-to-Image_Translation_CVPR_2022_paper.pdf" style="color:#71b07b;">CVPR ’22</a>, <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Unaligned_Image-to-Image_Translation_by_Learning_to_Reweight_ICCV_2021_paper.pdf" style="color:#71b07b;">ICCV ’21</a>].  
- **Causal representation learning**: extracting disentangled and structured latent factors from visual and multimodal data to enable systematic generalization [<a href="https://openreview.net/pdf?id=cW9Ttnm1aC" style="color:#71b07b;">ICML ’25</a>, <a href="https://arxiv.org/pdf/2306.12511.pdf" style="color:#71b07b;">ICLR ’25</a>, <a href="https://openreview.net/attachment?id=S8lfepB2fz&name=pdf" style="color:#71b07b;">AISTATS ’25</a>, <a href="https://arxiv.org/pdf/2402.05052" style="color:#71b07b;">ICML ’24</a>, <a href="https://proceedings.mlr.press/v162/kong22a/kong22a.pdf" style="color:#71b07b;">ICML ’22</a>].  
- **Counterfactual inference**: building frameworks that empower models to answer “what if” questions and reason about interventions [<a href="https://arxiv.org/pdf/2306.05751" style="color:#71b07b;">arXiv ’24</a>].

---

## News

- **[Aug. 2025]** I will be serving as an Area Chair for <a href="https://iclr.cc/" style="color:#71b07b;">ICLR 2026</a> — thanks for the invitation!  
- **[Jun. 2025]** I’m attending <a href="https://cvpr.thecvf.com/" style="color:#71b07b;">CVPR 2025</a> — see you in Nashville!  
- **[May. 2025]** Two papers accepted into <a href="https://icml.cc/" style="color:#71b07b;">ICML 2025</a>!  
- **[Apr. 2025]** One paper accepted as a <a href="https://cvpr.thecvf.com/" style="color:#71b07b;">CVPR 2025 Highlight</a>!  
- **[May. 2023]** My work at Adobe, <a href="https://arxiv.org/pdf/2212.05034.pdf" style="color:#71b07b;">SmartBrush (CVPR 2023, Highlight)</a>, is now integrated into Adobe’s flagship products — <a href="https://www.adobe.com/products/photoshop/generative-fill.html" style="color:#71b07b;">Photoshop and Firefly</a>!  
- **[Jan. 2023]** One paper, <a href="https://openreview.net/pdf?id=U2g8OGONA_V" style="color:#71b07b;">i-StyleGAN</a>, accepted as an <a href="https://iclr.cc/" style="color:#71b07b;">ICLR 2023 Spotlight</a>! One of the first works to connect identifiability with multidomain image generation.  

---

## Publications  

{% include_relative _includes/publications.md %}  

## Services  

{% include_relative _includes/services.md %}  
