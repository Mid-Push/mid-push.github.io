<div class="container">
    <div class="top-concept">
        <h1>Causal & Compositional Representations</h1>
        <p class="subtitle">How should we represent various types of concepts?</p>
        <p class="theory-question">How to capture the compositional and causal structures underlying these concepts?</p>
        <p class="research-flow">These research areas share a unified goal: building interpretable, identifiable representations</p>
    </div>

    <div class="arrows-down">
        <div class="arrow-down"></div>
        <div class="arrow-down"></div>
        <div class="arrow-down"></div>
        <div class="arrow-down"></div>
    </div>

    <div class="categories-grid">
        <div class="category-box">
            <h2 class="category-header">Causal Representation Theory</h2>
            <p class="category-tagline">Theoretical foundations and identifiability guarantees</p>
            
            <div class="subcategory">
                <p class="subcategory-title">Identifiable Latent Concepts:</p>
                <div class="paper-refs">
                    <a href="https://openreview.net/pdf?id=cW9Ttnm1aC" class="paper-ref">Nonparametric Identification of Latent Concepts <span class="venue">(ICML 2025)</span></a>
                    <a href="https://arxiv.org/pdf/2402.05052" class="paper-ref">Causal Representation Learning from Multiple Distributions <span class="venue">(ICML 2024)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">General Environments:</p>
                <div class="paper-refs">
                    <a href="https://openreview.net/attachment?id=S8lfepB2fz&name=pdf" class="paper-ref">Causal Representation Learning from General Environments under Nonparametric Mixing <span class="venue">(AISTATS 2025)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title"><span class="plus-sign">+</span> Disentangled Representations:</p>
                <div class="paper-refs">
                    <a href="https://arxiv.org/pdf/2503.00639" class="paper-ref">Synergy Between Sufficient Changes and Sparse Mixing for Disentangled Representation Learning <span class="venue">(ICLR 2025)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Counterfactual Reasoning:</p>
                <div class="paper-refs">
                    <a href="https://arxiv.org/pdf/2306.05751" class="paper-ref">Advancing Counterfactual Inference through Nonlinear Quantile Regression <span class="venue">(arXiv 2024)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Domain Adaptation:</p>
                <div class="paper-refs">
                    <a href="https://proceedings.mlr.press/v162/kong22a/kong22a.pdf" class="paper-ref">Partial Disentanglement for Domain Adaptation <span class="venue">(ICML 2022)</span></a>
                    <a href="http://proceedings.mlr.press/v80/xie18c/xie18c.pdf" class="paper-ref">Learning Semantic Representations for Unsupervised Domain Adaptation <span class="venue">(ICML 2018)</span></a>
                </div>
            </div>
        </div>

        <div class="category-box">
            <h2 class="category-header">Identifiable Image Translation</h2>
            <p class="category-tagline">Applying identifiability to generative models</p>
            
            <div class="subcategory">
                <p class="subcategory-title">Multi-Domain with Guarantees:</p>
                <div class="paper-refs">
                    <a href="https://openreview.net/pdf?id=U2g8OGONA_V" class="paper-ref">Multi-domain Image Generation and Translation with Identifiability Guarantees <span class="venue">(ICLR 2023 Spotlight)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title"><span class="plus-sign">+</span> Unpaired Translation:</p>
                <div class="paper-refs">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Unpaired_Image-to-Image_Translation_With_Shortest_Path_Regularization_CVPR_2023_paper.pdf" class="paper-ref">Unpaired Image-to-Image Translation with Shortest Path Regularization <span class="venue">(CVPR 2023)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title"><span class="plus-sign">+</span> Density-Based Methods:</p>
                <div class="paper-refs">
                    <a href="https://openreview.net/pdf?id=RNZ8JOmNaV4" class="paper-ref">Unsupervised Image-to-Image Translation with Density Changing Regularization <span class="venue">(NeurIPS 2022)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title"><span class="plus-sign">+</span> Spatial Consistency:</p>
                <div class="paper-refs">
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Maximum_Spatial_Perturbation_Consistency_for_Unpaired_Image-to-Image_Translation_CVPR_2022_paper.pdf" class="paper-ref">Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation <span class="venue">(CVPR 2022)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title"><span class="plus-sign">+</span> Learning to Reweight:</p>
                <div class="paper-refs">
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Unaligned_Image-to-Image_Translation_by_Learning_to_Reweight_ICCV_2021_paper.pdf" class="paper-ref">Unaligned Image-to-Image Translation by Learning to Reweight <span class="venue">(ICCV 2021)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Medical Image Generalization:</p>
                <div class="paper-refs">
                    <a href="https://arxiv.org/pdf/2206.13737.pdf" class="paper-ref">Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation <span class="venue">(MICCAI 2022)</span></a>
                </div>
            </div>
        </div>

        <div class="category-box">
            <h2 class="category-header">Vision-Language Concepts</h2>
            <p class="category-tagline">Extending identifiability to multimodal representations</p>
            
            <div class="subcategory">
                <p class="subcategory-title">Modular Vision-Language Alignment:</p>
                <div class="paper-refs">
                    <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.pdf" class="paper-ref">SmartCLIP: Modular Vision-Language Alignment with Identification Guarantees <span class="venue">(CVPR 2025 Highlight)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Controllable Generation with Concepts:</p>
                <div class="paper-refs">
                    <a href="https://openreview.net/pdf?id=hUHRTaTfvZ" class="paper-ref">Learning Vision and Language Concepts for Controllable Image Generation <span class="venue">(ICML 2025)</span></a>
                    <a href="https://arxiv.org/pdf/2502.02690" class="paper-ref">Learning Vision and Language Concepts for Controllable Image Generation <span class="venue">(arXiv 2025)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Text-Guided Image Manipulation:</p>
                <div class="paper-refs">
                    <a href="https://arxiv.org/pdf/2212.05034.pdf" class="paper-ref">SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model <span class="venue">(CVPR 2023 Highlight)</span></a>
                    <a href="https://arxiv.org/pdf/2312.03771" class="paper-ref">DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models <span class="venue">(arXiv 2024)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title"><span class="plus-sign">+</span> Diffusion Models:</p>
                <div class="paper-refs">
                    <a href="https://arxiv.org/pdf/2306.12511.pdf" class="paper-ref">Semi-Implicit Denoising Diffusion Models (SIDDMs) <span class="venue">(NeurIPS 2023)</span></a>
                </div>
            </div>
        </div>

        <div class="category-box">
            <h2 class="category-header">Post-Training of Models</h2>
            <p class="category-tagline">Optimizing and interpreting trained generative models</p>
            
            <div class="subcategory">
                <p class="subcategory-title">Interpretability of Generative Models:</p>
                <div class="paper-refs">
                    <a href="#" class="paper-ref">Understanding Latent Structures in Diffusion Models <span class="venue">(Upcoming)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Policy Optimization for Diffusion Models:</p>
                <div class="paper-refs">
                    <a href="#" class="paper-ref">Reinforcement Learning for Diffusion Policy Optimization <span class="venue">(Upcoming)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Policy Optimization for Language Models:</p>
                <div class="paper-refs">
                    <a href="#" class="paper-ref">Advanced RLHF Techniques for Language Model Alignment <span class="venue">(Upcoming)</span></a>
                </div>
            </div>

            <div class="subcategory">
                <p class="subcategory-title">Reinforcement Learning for Visual Generation:</p>
                <div class="paper-refs">
                    <a href="#" class="paper-ref">RL-Based Fine-Tuning for Image Generation Quality <span class="venue">(Upcoming)</span></a>
                </div>
            </div>
        </div>
    </div>
</div>
